{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0x00. 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，互联网上的搜索引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。\n",
    "\n",
    "                                                                                   -- wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0x01. 原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF的主要思想是一个词在一个文档中出现的频率大，在所有文档中出现的频率小，那么这个词就有比较大的分类能力，可以用来分类。通过这种方式算法会倾向分类能力强的重要的词，忽视常用词。\n",
    "\n",
    "TF-IDF算法分为两个部分，第一个部分是TF(Term Frequency)，计算词频。公式如下：\n",
    "\n",
    "$$tf_{i,j} = \\frac{n_{i,j}}{\\sum_{k=1}^{N} n_{kj}}$$ \n",
    "\n",
    "其中$tf_{i,j}$代表第j个文档中第i个词出现的频率，$n_{i,j}$ 代表第j篇文档中第i个词出现的总次数，$\\sum_{k=1}^{N} n_{kj}$ 代表文档j中所有单词出现的次数总和。\n",
    "\n",
    "TF-IDF算法第二部分计算IDF(Inverse Document Frequency, IDF)，反向文档频率。也就是计算所有文档与出现被衡量单词的文档数量之间的关系。公式如下：\n",
    "\n",
    "$$idf_i = \\log \\frac{\\left | D \\right |}{\\left | \\{j : t_i \\in d_j\\}\\right|}$$\n",
    "\n",
    "其中$idf_i$表示单词i的反向文档频率，其值是文档总数 $\\left | D \\right|$ 与包含单词i的文档数 $\\left | \\{j : t_i \\in d_j\\}\\right|$ 之比的对数值。\n",
    "\n",
    "TF-IDF算法通过TF计算，表征了单词在一篇文档中的重要程度，单词i出现的次数越多TF值越大，也就说明单词i越重要。相反，在所有文档中单词i出现的次数越多则IDF值越小，说明单词i的区分度较低，分类能力差，重要性也就越低。综合TF值与IDF值最终得到单词i总体的重要程度。\n",
    "\n",
    "$$tfidf_i = tf_i \\times idf_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0x02. 示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面通过两条简短的例句对TF-IDF算法进行演示\n",
    "\n",
    ">1. These art works bear witness to the creativeness of the Chinese people.\n",
    ">2. These islands have always been under Chinese jurisdiction. This is known to all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc:These art works bear witness to the creativeness to the Chinese people.\n",
      "These\t0.0\n",
      "art\t0.057762265046662105\n",
      "works\t0.057762265046662105\n",
      "bear\t0.057762265046662105\n",
      "witness\t0.057762265046662105\n",
      "to\t0.0\n",
      "the\t0.0\n",
      "creativeness\t0.057762265046662105\n",
      "to\t0.0\n",
      "the\t0.0\n",
      "Chinese\t0.0\n",
      "people\t0.057762265046662105\n",
      "\n",
      "Doc:These islands have always been under the Chinese jurisdiction. This is known to all.\n",
      "These\t0.0\n",
      "islands\t0.049510512897138946\n",
      "have\t0.049510512897138946\n",
      "always\t0.049510512897138946\n",
      "been\t0.049510512897138946\n",
      "under\t0.049510512897138946\n",
      "the\t0.0\n",
      "Chinese\t0.0\n",
      "jurisdiction\t0.049510512897138946\n",
      "This\t0.049510512897138946\n",
      "is\t0.049510512897138946\n",
      "known\t0.049510512897138946\n",
      "to\t0.0\n",
      "all\t0.049510512897138946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 示例文档\n",
    "documents = [\n",
    "    \"These art works bear witness to the creativeness to the Chinese people.\",\n",
    "    \"These islands have always been under the Chinese jurisdiction. This is known to all.\"\n",
    "]\n",
    "\n",
    "# 分词\n",
    "function token(doc, dilimiter)\n",
    "    return split(replace(doc, \".\" ,\"\"), dilimiter)\n",
    "end\n",
    "\n",
    "# 计算词频\n",
    "function frequency(word, words)\n",
    "    count = 0\n",
    "    for w in words\n",
    "        if w == word\n",
    "            count += 1\n",
    "        end\n",
    "    end\n",
    "    return count\n",
    "end\n",
    "\n",
    "# 计算tf\n",
    "function tf(word, words)\n",
    "    return frequency(word, words) / length(words)\n",
    "end\n",
    "\n",
    "# 计算idf\n",
    "function idf(word, documents)\n",
    "    count = 0\n",
    "    for doc in documents\n",
    "        if contains(doc, word)\n",
    "            count += 1\n",
    "        end\n",
    "    end\n",
    "    return log(length(documents) / (count == 0 ? 1 : count))\n",
    "end\n",
    "\n",
    "# 计算 tf-idf\n",
    "function tf_idf(documents)\n",
    "    for doc in documents\n",
    "        words = token(doc, \" \")\n",
    "        println(\"Doc:\", doc)\n",
    "        for w in words\n",
    "            tfidf = tf(w, words) * idf(w, documents)\n",
    "            println(w, \"\\t\", tfidf)\n",
    "        end\n",
    "        println()\n",
    "    end\n",
    "end\n",
    "\n",
    "tf_idf(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面例子的输出结果我们可以发现 These, the, Chinese, to 这四个词同时出现在了两个文档中，他们的TF-IDF值为0，没有分类能力，重要性为0. 这也就验证了TF-IDF算法的效果。倾向于分类能力强的单词，也就是在一个文档中出现的频率大，在所有的文档中出现的频率小的单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0x03. 总结\n",
    "TF-IDF通常可以用在文章关键词提取、计算文章相似度等很多问题中，是自然语言处理领域十分重要的算法。\n",
    "\n",
    "另外本篇文章只是浅尝辄止，关于该算法深入的数学原理并没有深入探讨。示例部分只展示了算法的原理与效果，代码并不高效，使用过程中还需要进行优化。此外，使用TF-IDF算法时还可以通过对停用词、同义词、拼写错误词进行处理进行进一步优化，得到更好的效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
